{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "from collections import Counter\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "WORD_FREQUENCY_FILE_FULL_PATH = \"analysis.vocab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVocabulary:\n",
    "    \n",
    "    def __init__(self, vocabulary, wordFrequencyFilePath):\n",
    "        self.vocabulary = vocabulary\n",
    "        self.WORD_FREQUENCY_FILE_FULL_PATH = wordFrequencyFilePath\n",
    "        self.input_word_index = {}\n",
    "        self.reverse_input_word_index = {}\n",
    "        \n",
    "        self.input_word_index[\"START\"] = 1\n",
    "        self.input_word_index[\"UNKOWN\"] = -1\n",
    "        self.MaxSentenceLength = None\n",
    "        \n",
    "    def PrepareVocabulary(self,reviews):\n",
    "        self._prepare_Word_Frequency_Count_File(reviews)\n",
    "        self._create_Vocab_Indexes()\n",
    "        \n",
    "        self.MaxSentenceLength = max([len(txt.split(\" \")) for txt in reviews])\n",
    "      \n",
    "    def Get_Top_Words(self, number_words = None):\n",
    "        if number_words == None:\n",
    "            number_words = self.vocabulary\n",
    "        \n",
    "        chars = json.loads(open(self.WORD_FREQUENCY_FILE_FULL_PATH).read())\n",
    "        counter = Counter(chars)\n",
    "        most_popular_words = {key for key, _value in counter.most_common(number_words)}\n",
    "        return most_popular_words\n",
    "    \n",
    "    def _prepare_Word_Frequency_Count_File(self,reviews):\n",
    "        counter = Counter()    \n",
    "        for s in reviews:\n",
    "            counter.update(s.split(\" \"))\n",
    "            \n",
    "        with open(self.WORD_FREQUENCY_FILE_FULL_PATH, 'w') as output_file:\n",
    "            output_file.write(json.dumps(counter))\n",
    "                 \n",
    "    def _create_Vocab_Indexes(self):\n",
    "        INPUT_WORDS = self.Get_Top_Words(self.vocabulary)\n",
    "\n",
    "        #word to int\n",
    "        #self.input_word_index = dict(\n",
    "        #    [(word, i) for i, word in enumerate(INPUT_WORDS)])\n",
    "        for i, word in enumerate(INPUT_WORDS):\n",
    "            self.input_word_index[word] = i\n",
    "        \n",
    "        #int to word\n",
    "        #self.reverse_input_word_index = dict(\n",
    "        #    (i, word) for word, i in self.input_word_index.items())\n",
    "        for word, i in self.input_word_index.items():\n",
    "            self.reverse_input_word_index[i] = word\n",
    "\n",
    "        #self.input_word_index = input_word_index\n",
    "        #self.reverse_input_word_index = reverse_input_word_index\n",
    "        #seralize.dump(config.DATA_FOLDER_PATH+\"input_word_index.p\",input_word_index)\n",
    "        #seralize.dump(config.DATA_FOLDER_PATH+\"reverse_input_word_index.p\",reverse_input_word_index)\n",
    "        \n",
    "        \n",
    "    def TransformSentencesToId(self, sentences):\n",
    "        vectors = []\n",
    "        for r in sentences:\n",
    "            words = r.split(\" \")\n",
    "            vector = np.zeros(len(words))\n",
    "\n",
    "            for t, word in enumerate(words):\n",
    "                if word in self.input_word_index:\n",
    "                    vector[t] = self.input_word_index[word]\n",
    "                else:\n",
    "                    pass\n",
    "                    #vector[t] = 2 #unk\n",
    "            vectors.append(vector)\n",
    "            \n",
    "        return vectors\n",
    "    \n",
    "    def ReverseTransformSentencesToId(self, sentences):\n",
    "        vectors = []\n",
    "        for r in sentences:\n",
    "            words = r.split(\" \")\n",
    "            vector = np.zeros(len(words))\n",
    "\n",
    "            for t, word in enumerate(words):\n",
    "                if word in self.input_word_index:\n",
    "                    vector[t] = self.input_word_index[word]\n",
    "                else:\n",
    "                    pass\n",
    "                    #vector[t] = 2 #unk\n",
    "            vectors.append(vector)\n",
    "            \n",
    "        return vectors\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download DataSet\n",
    "#http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "#https://www.liip.ch/en/blog/sentiment-detection-with-keras-word-embeddings-and-lstm-deep-learning-networks\n",
    "import os\n",
    "\n",
    "def GetTextFilePathsInDirectory(directory):\n",
    "    files = []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".txt\"):\n",
    "            filePath = os.path.join(directory, file)\n",
    "            files.append(filePath)\n",
    "    return files\n",
    "\n",
    "def GetLinesFromTextFile(filePath):\n",
    "    with open(filePath,\"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [line.strip() for line in f]\n",
    "    return lines\n",
    "\n",
    "\n",
    "def RemoveStopWords(line, stopwords):\n",
    "    words = []\n",
    "    for word in line.split(\" \"):\n",
    "        word = word.strip()\n",
    "        if word not in stopwords and word != \"\" and word != \"&\":\n",
    "            words.append(word)\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "import re\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "#https://gist.github.com/aaronkub/257a1bd9215da3a7221148600d849450#file-clean_movie_reviews-py\n",
    "def preprocess_reviews(reviews):\n",
    "    default_stop_words = nltk.corpus.stopwords.words('english')\n",
    "    stopwords = set(default_stop_words)\n",
    "    \n",
    "    reviews = [REPLACE_NO_SPACE.sub(\"\", line.lower()) for line in reviews]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(\" \", line) for line in reviews]\n",
    "    reviews = [RemoveStopWords(line,stopwords) for line in reviews]\n",
    "    \n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'large test'"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_stop_words = nltk.corpus.stopwords.words('english')\n",
    "stopwords = set(default_stop_words)\n",
    "RemoveStopWords(\"this is a very large test\",stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Prepare Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_files = GetTextFilePathsInDirectory(\"aclImdb/train/pos/\")\n",
    "negative_files = GetTextFilePathsInDirectory(\"aclImdb/train/neg/\")\n",
    "\n",
    "reviews_positive = []\n",
    "for i in range(0,500):\n",
    "    reviews_positive.extend(GetLinesFromTextFile(positive_files[i]))\n",
    "    \n",
    "reviews_negative = []\n",
    "for i in range(0,500):\n",
    "    reviews_negative.extend(GetLinesFromTextFile(negative_files[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Review---> This isn't the comedic Robin Williams, nor is it the quirky/insane Robin Williams of recent thriller fame. This is a hybrid of the classic drama without over-dramatization, mixed with Robin's new love of the thriller. But this isn't a thriller, per se. This is more a mystery/suspense vehicle through which Williams attempts to locate a sick boy and his keeper.<br /><br />Also starring Sandra Oh and Rory Culkin, this Suspense Drama plays pretty much like a news report, until William's character gets close to achieving his goal.<br /><br />I must say that I was highly entertained, though this movie fails to teach, guide, inspect, or amuse. It felt more like I was watching a guy (Williams), as he was actually performing the actions, from a third person perspective. In other words, it felt real, and I was able to subscribe to the premise of the story.<br /><br />All in all, it's worth a watch, though it's definitely not Friday/Saturday night fare.<br /><br />It rates a 7.7/10 from...<br /><br />the Fiend :.\n",
      "\n",
      "Negative Review---> \"It appears that many critics find the idea of a Woody Allen drama unpalatable.\" And for good reason: they are unbearably wooden and pretentious imitations of Bergman. And let's not kid ourselves: critics were mostly supportive of Allen's Bergman pretensions, Allen's whining accusations to the contrary notwithstanding. What I don't get is this: why was Allen generally applauded for his originality in imitating Bergman, but the contemporaneous Brian DePalma was excoriated for \"ripping off\" Hitchcock in his suspense/horror films? In Robin Wood's view, it's a strange form of cultural snobbery. I would have to agree with that.\n",
      "\n",
      "Processed Positive Review---> isnt comedic robin williams quirky insane robin williams recent thriller fame hybrid classic drama without dramatization mixed robins new love thriller isnt thriller per se mystery suspense vehicle williams attempts locate sick boy keeper also starring sandra oh rory culkin suspense drama plays pretty much like news report williams character gets close achieving goal must say highly entertained though movie fails teach guide inspect amuse felt like watching guy williams actually performing actions third person perspective words felt real able subscribe premise story worth watch though definitely friday saturday night fare rates 77 10 fiend\n",
      "\n",
      "Processed Negative Review---> appears many critics find idea woody allen drama unpalatable good reason unbearably wooden pretentious imitations bergman lets kid critics mostly supportive allens bergman pretensions allens whining accusations contrary notwithstanding dont get allen generally applauded originality imitating bergman contemporaneous brian depalma excoriated ripping hitchcock suspense horror films robin woods view strange form cultural snobbery would agree\n"
     ]
    }
   ],
   "source": [
    "print(\"Positive Review---> {0}\".format(reviews_positive[5]))\n",
    "print()\n",
    "print(\"Negative Review---> {0}\".format(reviews_negative[5]))\n",
    "\n",
    "print()\n",
    "reviews_positive = preprocess_reviews(reviews_positive)\n",
    "print(\"Processed Positive Review---> {0}\".format(reviews_positive[5]))\n",
    "\n",
    "reviews_negative = preprocess_reviews(reviews_negative)\n",
    "print(\"Processed Negative Review---> {0}\".format(reviews_negative[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Labeled DataSet</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('first read armistead maupins story taken human drama displayed gabriel one cares loves said given film version excellent story expected see past gloss hollywood writer armistead maupin director patrick stettner truly succeeded right amount restraint robin williams captures fragile essence gabriel lets us see struggle issues trust personnel lifejess world around himdonna introduced players drama reminded nothing ever seems smallest event change lives irrevocably request review book written young man turns life changing event helps gabriel find strength within carry move forward bad people avoid film say average american probably think robin williams serious role didnt work please give movie chance robin williams touches darkness must find go better people like movie one hour photo stepped actor made another quality piece art oh forget believe bobby cannavale jess steals every scene 1940s leading man looks screen presence hacks opinion could carry movie right s~',\n",
       " 1.0)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Reviews_Labeled = list(zip(reviews_positive, np.ones(len(reviews_positive))))\n",
    "Reviews_Labeled.extend(list(zip(reviews_negative, np.zeros(len(reviews_negative)))))\n",
    "Reviews_Labeled[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Prepare Vocabulary</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOP_WORDS = 500\n",
    "vocab = MyVocabulary(TOP_WORDS,\"analysis.vocab\")\n",
    "\n",
    "reviews_text = [line[0] for line in Reviews_Labeled]\n",
    "vocab.PrepareVocabulary(reviews_text)\n",
    "\n",
    "#vocab.Get_Top_Words(10)\n",
    "\n",
    "vocab.input_word_index[\"START\"]\n",
    "#vocab.input_word_index[\"UNKOWN\"]\n",
    "#vocab.input_word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Integer Encode Words</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([354.,   0.,   0., 418.,   0.,   0.,   0., 418.,   0.,   0.,   0.,\n",
      "         0., 393., 269., 344.,   0.,   0.,   0., 179., 171.,   0., 354.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0., 418.,   0.,   0.,   0., 396.,\n",
      "         0., 491.,   0.,   0., 496.,   0.,   0.,   0., 269., 360.,  62.,\n",
      "       475., 338.,   0.,   0., 418.,  51., 197., 215.,   0.,   0., 431.,\n",
      "       428.,   1.,   0., 433., 423.,   0.,   0.,   0.,   0.,   0., 268.,\n",
      "       338., 424., 427., 418., 299.,   0.,   0.,   0.,  21.,   0.,   0.,\n",
      "       268., 304., 454.,   0.,   0., 307.,  82., 144., 433., 348.,   0.,\n",
      "         0., 298.,   0.,   0.,   0., 165.,   0.]), 1.0)\n"
     ]
    }
   ],
   "source": [
    "reviews,labels=zip(*Reviews_Labeled)\n",
    "reviews_int = vocab.TransformSentencesToId(reviews)\n",
    "\n",
    "Reviews_Labeled_Int = list(zip(reviews_int,labels))\n",
    "#print(len(Reviews_Labeled_Int))\n",
    "print(Reviews_Labeled_Int[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Split Train and Test </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(Reviews_Labeled_Int, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pad Sentences</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = list(zip(*train))\n",
    "X_test, y_test = list(zip(*test))\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "#max_review_length = vocab.MaxSentenceLength\n",
    "max_review_length = 500\n",
    "\n",
    "# Truncate and pad the review sequences \n",
    "from keras.preprocessing import sequence \n",
    "max_review_length = 500 \n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length) \n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(800, 500)\n",
      "(200, 500)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(800,)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(type(X_test))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(type(y_train))\n",
    "print(type(y_test))\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 500, 32)           16000     \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 69,301\n",
      "Trainable params: 69,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding, LSTM\n",
    "import keras \n",
    "\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(TOP_WORDS, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/3\n",
      "800/800 [==============================] - 10s 13ms/step - loss: 0.6930 - acc: 0.4950 - val_loss: 0.6927 - val_acc: 0.4750\n",
      "Epoch 2/3\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6895 - acc: 0.6275 - val_loss: 0.6884 - val_acc: 0.6400\n",
      "Epoch 3/3\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6793 - acc: 0.7037 - val_loss: 0.6622 - val_acc: 0.5100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2db2f6273c8>"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "w = [\"a\",\"b\",\"c\"]\n",
    "\n",
    "for i,w in enumerate(w):\n",
    "    print(i+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
