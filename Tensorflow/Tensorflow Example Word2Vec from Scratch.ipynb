{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCES = [\"machine learning engineers can build great data models\",\n",
    "             \"the more data you have the better your model\",\n",
    "             \"these predictions sound right, but it is all about your data\",\n",
    "             \"your data can provide great value\"\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import numpy as np\n",
    "class Vocabulary:\n",
    "    \n",
    "    def __init__(self, vocabulary, wordFrequencyFilePath):\n",
    "        self.vocabulary = vocabulary\n",
    "        self.BAG_OF_WORDS_FILE_FULL_PATH = wordFrequencyFilePath\n",
    "        self.input_word_index = {}\n",
    "        self.reverse_input_word_index = {}\n",
    "        \n",
    "        self.input_word_index[\"START\"] = 1\n",
    "        self.input_word_index[\"UNKOWN\"] = -1\n",
    "        self.MaxSentenceLength = None\n",
    "        \n",
    "    def PrepareVocabulary(self,reviews):\n",
    "        self._prepare_Bag_of_Words_File(reviews)\n",
    "        self._create_Vocab_Indexes()\n",
    "        \n",
    "        self.MaxSentenceLength = max([len(txt.split(\" \")) for txt in reviews])\n",
    "      \n",
    "    def Get_Top_Words(self, number_words = None):\n",
    "        if number_words == None:\n",
    "            number_words = self.vocabulary\n",
    "        \n",
    "        chars = json.loads(open(self.BAG_OF_WORDS_FILE_FULL_PATH).read())\n",
    "        counter = Counter(chars)\n",
    "        most_popular_words = {key for key, _value in counter.most_common(number_words)}\n",
    "        return most_popular_words\n",
    "    \n",
    "    def _prepare_Bag_of_Words_File(self,reviews):\n",
    "        counter = Counter()    \n",
    "        for s in reviews:\n",
    "            counter.update(s.split(\" \"))\n",
    "            \n",
    "        with open(self.BAG_OF_WORDS_FILE_FULL_PATH, 'w') as output_file:\n",
    "            output_file.write(json.dumps(counter))\n",
    "                 \n",
    "    def _create_Vocab_Indexes(self):\n",
    "        INPUT_WORDS = self.Get_Top_Words(self.vocabulary)\n",
    "\n",
    "        #word to int\n",
    "        #self.input_word_index = dict(\n",
    "        #    [(word, i) for i, word in enumerate(INPUT_WORDS)])\n",
    "        for i, word in enumerate(INPUT_WORDS):\n",
    "            self.input_word_index[word] = i\n",
    "        \n",
    "        #int to word\n",
    "        #self.reverse_input_word_index = dict(\n",
    "        #    (i, word) for word, i in self.input_word_index.items())\n",
    "        for word, i in self.input_word_index.items():\n",
    "            self.reverse_input_word_index[i] = word\n",
    "\n",
    "        #self.input_word_index = input_word_index\n",
    "        #self.reverse_input_word_index = reverse_input_word_index\n",
    "        #seralize.dump(config.DATA_FOLDER_PATH+\"input_word_index.p\",input_word_index)\n",
    "        #seralize.dump(config.DATA_FOLDER_PATH+\"reverse_input_word_index.p\",reverse_input_word_index)\n",
    "        \n",
    "        \n",
    "    def _word_to_One_Hot_Vector(self, word):\n",
    "        vector = np.zeros(self.vocabulary)\n",
    "        vector[vocab.input_word_index[word]] = 1\n",
    "        return vector\n",
    "        \n",
    "    def TransformSentencesToId(self, sentences):\n",
    "        vectors = []\n",
    "        for r in sentences:\n",
    "            words = r.split(\" \")\n",
    "            vector = np.zeros(len(words))\n",
    "\n",
    "            for t, word in enumerate(words):\n",
    "                if word in self.input_word_index:\n",
    "                    vector[t] = self.input_word_index[word]\n",
    "                else:\n",
    "                    pass\n",
    "                    #vector[t] = 2 #unk\n",
    "            vectors.append(vector)\n",
    "            \n",
    "        return vectors\n",
    "    \n",
    "    def ReverseTransformSentencesToId(self, sentences):\n",
    "        vectors = []\n",
    "        for r in sentences:\n",
    "            words = r.split(\" \")\n",
    "            vector = np.zeros(len(words))\n",
    "\n",
    "            for t, word in enumerate(words):\n",
    "                if word in self.input_word_index:\n",
    "                    vector[t] = self.input_word_index[word]\n",
    "                else:\n",
    "                    pass\n",
    "                    #vector[t] = 2 #unk\n",
    "            vectors.append(vector)\n",
    "            \n",
    "        return vectors\n",
    "    \n",
    "    \n",
    "    def Get_SkipGram_Target_Words(self, sentences, WINDOW_SIZE = 5):\n",
    "        SKIP_GRAM_INPUT_WORD_LIST = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_tokenized = sentence.split(\" \")\n",
    "            \n",
    "            for index, target_word in enumerate(sentence_tokenized):\n",
    "                FROM_INDEX = max(index-WINDOW_SIZE,0)\n",
    "                TO_INDEX = min(index+1+WINDOW_SIZE,len(sentence_tokenized))\n",
    "\n",
    "                for contextWord in sentence_tokenized[FROM_INDEX:TO_INDEX]:\n",
    "                    if contextWord != target_word:\n",
    "                        SKIP_GRAM_INPUT_WORD_LIST.append((target_word,contextWord))\n",
    "                    \n",
    "        return SKIP_GRAM_INPUT_WORD_LIST\n",
    "    \n",
    "    \n",
    "    def Get_SkipGram_Target_Words_OneHotEncoded_XY(self, sentences, WINDOW_SIZE = 5):\n",
    "        Skip_Gram_Target_Words = self.Get_SkipGram_Target_Words(sentences, WINDOW_SIZE)\n",
    "        \n",
    "        X,Y = [],[]\n",
    "        \n",
    "        for target_word, context_word in Skip_Gram_Target_Words:\n",
    "            X.append(self._word_to_One_Hot_Vector(target_word))\n",
    "            Y.append(self._word_to_One_Hot_Vector(context_word))\n",
    "            \n",
    "        return np.asarray(X), np.asarray(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary of 26 words\n"
     ]
    }
   ],
   "source": [
    "VOCABULARY_SIZE = 26\n",
    "vocab = Vocabulary(VOCABULARY_SIZE,\"words.vocab\")\n",
    "vocab.PrepareVocabulary(SENTENCES)\n",
    "vocab.Get_Top_Words(5)\n",
    "print(\"Vocabulary of {0} words\".format(len(vocab.Get_Top_Words())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'machine learning engineers can build great data models'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SENTENCES[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "Skip_Gram_Target_Words = vocab.Get_SkipGram_Target_Words(SENTENCES, WINDOW_SIZE=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine\n",
      "machine\n",
      "machine\n",
      "learning\n",
      "learning\n",
      "learning\n",
      "learning\n",
      "engineers\n",
      "engineers\n",
      "engineers\n",
      "engineers\n",
      "engineers\n",
      "can\n",
      "can\n",
      "can\n",
      "can\n",
      "can\n",
      "can\n",
      "build\n",
      "build\n",
      "build\n",
      "build\n",
      "build\n",
      "build\n",
      "great\n",
      "great\n",
      "great\n",
      "great\n",
      "great\n",
      "data\n",
      "data\n",
      "data\n",
      "data\n",
      "models\n",
      "models\n",
      "models\n",
      "the\n",
      "the\n",
      "the\n",
      "more\n",
      "more\n",
      "more\n",
      "more\n",
      "data\n",
      "data\n",
      "data\n",
      "data\n",
      "data\n",
      "you\n",
      "you\n",
      "you\n",
      "you\n",
      "you\n",
      "you\n",
      "have\n",
      "have\n",
      "have\n",
      "have\n",
      "have\n",
      "have\n",
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "better\n",
      "better\n",
      "better\n",
      "better\n",
      "better\n",
      "your\n",
      "your\n",
      "your\n",
      "your\n",
      "model\n",
      "model\n",
      "model\n",
      "these\n",
      "these\n",
      "these\n",
      "predictions\n",
      "predictions\n",
      "predictions\n",
      "predictions\n",
      "sound\n",
      "sound\n",
      "sound\n",
      "sound\n",
      "sound\n",
      "right,\n",
      "right,\n",
      "right,\n",
      "right,\n",
      "right,\n",
      "right,\n",
      "but\n",
      "but\n",
      "but\n",
      "but\n",
      "but\n",
      "but\n",
      "it\n",
      "it\n",
      "it\n",
      "it\n",
      "it\n",
      "it\n",
      "is\n",
      "is\n",
      "is\n",
      "is\n",
      "is\n",
      "is\n",
      "all\n",
      "all\n",
      "all\n",
      "all\n",
      "all\n",
      "all\n",
      "about\n",
      "about\n",
      "about\n",
      "about\n",
      "about\n",
      "your\n",
      "your\n",
      "your\n",
      "your\n",
      "data\n",
      "data\n",
      "data\n",
      "your\n",
      "your\n",
      "your\n",
      "data\n",
      "data\n",
      "data\n",
      "data\n",
      "can\n",
      "can\n",
      "can\n",
      "can\n",
      "can\n",
      "provide\n",
      "provide\n",
      "provide\n",
      "provide\n",
      "provide\n",
      "great\n",
      "great\n",
      "great\n",
      "great\n",
      "value\n",
      "value\n",
      "value\n"
     ]
    }
   ],
   "source": [
    "for target, context in Skip_Gram_Target_Words:\n",
    "    print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = vocab.Get_SkipGram_Target_Words_OneHotEncoded_XY(SENTENCES,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 26)\n",
      "(112, 26)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 5\n",
    "\n",
    "# Inputs\n",
    "X = tf.placeholder(\"float\", shape=[None, VOCABULARY_SIZE])\n",
    "y = tf.placeholder(\"float\", shape=[None, VOCABULARY_SIZE])\n",
    "\n",
    "# Dictionary of Weights and Biases\n",
    "weights = {\n",
    "  'W1': tf.Variable(tf.random_normal([VOCABULARY_SIZE, EMBEDDING_DIM])),\n",
    "  'W2': tf.Variable(tf.random_normal([EMBEDDING_DIM, VOCABULARY_SIZE])),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "  'b1': tf.Variable(tf.random_normal([EMBEDDING_DIM])),\n",
    "  'b2': tf.Variable(tf.random_normal([VOCABULARY_SIZE])),\n",
    "}\n",
    "\n",
    "\n",
    "# Model Forward Propagation step\n",
    "def forward_propagation(x):\n",
    "    hidden_1 = tf.add(tf.matmul(x, weights['W1']), biases['b1'])   \n",
    "    out_layer = tf.add(tf.matmul(hidden_1, weights['W2']), biases['b2'])\n",
    "    \n",
    "    softmax_out = tf.nn.softmax(out_layer)    \n",
    "    return softmax_out\n",
    "\n",
    "#cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=yhat))\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "yhat = forward_propagation(X)\n",
    "ypredict = tf.argmax(yhat, axis=1)\n",
    "\n",
    "# Backward propagation\n",
    "learning_rate = 0.2\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=yhat))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1, train accuracy = 6.25%\n",
      "Epoch = 51, train accuracy = 21.43%\n",
      "Epoch = 101, train accuracy = 21.43%\n",
      "Epoch = 151, train accuracy = 23.21%\n",
      "Epoch = 201, train accuracy = 24.11%\n",
      "Epoch = 251, train accuracy = 24.11%\n",
      "Epoch = 301, train accuracy = 24.11%\n",
      "Epoch = 351, train accuracy = 24.11%\n",
      "Epoch = 401, train accuracy = 24.11%\n",
      "Epoch = 451, train accuracy = 25.00%\n",
      "Epoch = 501, train accuracy = 25.00%\n",
      "Epoch = 551, train accuracy = 25.00%\n",
      "Epoch = 601, train accuracy = 25.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-184-8cd41d0c9603>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;31m#cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(yhat), reduction_indices=[1]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0msummary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m50\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mjbol\\anaconda3\\envs\\pabase\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mjbol\\anaconda3\\envs\\pabase\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mjbol\\anaconda3\\envs\\pabase\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mjbol\\anaconda3\\envs\\pabase\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mjbol\\anaconda3\\envs\\pabase\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mjbol\\anaconda3\\envs\\pabase\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "from datetime import datetime\n",
    "startTime = datetime.now()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    #writer.add_graph(sess.graph)\n",
    "    #EPOCHS\n",
    "    for epoch in range(1000):\n",
    "        #Stochasting Gradient Descent\n",
    "        for i in range(len(X_train)):\n",
    "            #cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(yhat), reduction_indices=[1]))\n",
    "            summary = sess.run(train_op, feed_dict={X: X_train[i: i + 1], y: Y_train[i: i + 1]})\n",
    "        \n",
    "        if epoch % 50 == 0:\n",
    "            train_accuracy = np.mean(np.argmax(Y_train, axis=1) == sess.run(ypredict, feed_dict={X: X_train, y: Y_train}))\n",
    "            #cross_entropy_loss_val = sess.run(cross_entropy_loss, feed_dict={X: X_train, y: Y_train})\n",
    "            \n",
    "            print(\"Epoch = %d, train accuracy = %.2f%%\" % (epoch + 1, 100. * train_accuracy))\n",
    "            #print(\"Epoch = %d, train accuracy = %.2f%%, train accuracy = %.2f%%\" % (epoch + 1, 100. * train_accuracy, cross_entropy_loss_val))\n",
    "\n",
    "    sess.close()\n",
    "print(\"Time taken:\", datetime.now() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
