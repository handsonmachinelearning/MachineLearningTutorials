{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 2.3.2\n",
      "PySpark Version: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName(\"Vectorizer\").setMaster(\"spark://10.102.2.122:7077\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "print(\"Spark Version: \" + sc.version)\n",
    "print(\"PySpark Version: \" + pyspark.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Functions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list, collect_set, split\n",
    "from pyspark.sql.functions import upper,col,udf,concat, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType, DateType\n",
    "def GetSchema():\n",
    "    schema = StructType([\n",
    "        StructField(\"PATIENT_NOTE_KEY\", IntegerType(), True),\n",
    "        StructField(\"complaint_tokens_str\", StringType(), True)])\n",
    "    return schema\n",
    "\n",
    "import os\n",
    "def Collect_Raw_Data(sqlContext):\n",
    "    df_files = sqlContext.read.csv(\"E:/TEMP/OUTPUT/ChiefComplaint_CleanedText/*.csv\",header=False,schema=GetSchema(),sep=\"|\")\n",
    "    return df_files\n",
    "\n",
    "import os\n",
    "def Collect_Raw_Data_Single(sqlContext):\n",
    "    df_files = sqlContext.read.csv(\"E:/TEMP/OUTPUT/ChiefComplaint_SingleFile/*.csv\",header=False,schema=GetSchema(),sep=\"|\")\n",
    "    return df_files\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer, CountVectorizerModel\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "import os\n",
    "\n",
    "class Vocabulary(object):\n",
    "\n",
    "    def __init__(self, comp_cv = None, icd_cv = None):\n",
    "        if comp_cv is not None and icd_cv is not None:\n",
    "            self.setComp_CV(comp_cv)\n",
    "            self.setICD_CV(icd_cv)\n",
    "\n",
    "    def setComp_CV(self, comp_cv):\n",
    "        self.comp_cv = comp_cv\n",
    "        self.comp_dict = Save_Model_Vocab(self.comp_cv, 'comp_cv')\n",
    "\n",
    "    def setICD_CV(self, icd_cv):\n",
    "        self.icd_cv = icd_cv\n",
    "        self.icd_dict = Save_Model_Vocab(self.icd_cv, 'icd_cv')\n",
    "\n",
    "    def saveVocab(self):\n",
    "        Save_Model_Vocab(self.comp_cv, \"comp_cv\")\n",
    "        self.comp_cv.write().overwrite().save('vocab\\\\complaint_vocab')\n",
    "        Save_Model_Vocab(self.icd_cv, \"icd_cv\")\n",
    "        self.icd_cv.write().overwrite().save('vocab\\\\icd_vocab')\n",
    "\n",
    "    def loadVocab(self):\n",
    "        try:\n",
    "            self.comp_cv = CountVectorizerModel.load(\"vocab\\\\complaint_vocab\")\n",
    "            self.icd_cv = CountVectorizerModel.load(\"vocab\\\\icd_vocab\")\n",
    "        except:\n",
    "            print('No spark context initialized so not loading vocabulary models')\n",
    "        self.comp_dict = {'int':json.load(open(\"vocab\\\\comp_cv_model_int-token.txt\")), 'token': json.load(open(\"vocab\\\\comp_cv_model_token-int.txt\"))}\n",
    "        self.icd_dict = {'int':json.load(open(\"vocab\\\\icd_cv_model_int-token.txt\")), 'token':json.load(open(\"vocab\\\\icd_cv_model_token-int.txt\"))}\n",
    "\n",
    "    def setVocab(self, comp_cv = None, icd_cv = None):\n",
    "        self.setComp_CV(comp_cv)\n",
    "        self.setICD_CV(icd_cv)\n",
    "\n",
    "    def getVocab(self):\n",
    "        return [self.comp_cv, self.icd_cv]\n",
    "\n",
    "    def getDict(self):\n",
    "        return {\"comp\": self.comp_dict, \"icd\":self.icd_dict}\n",
    "\n",
    "    def fitVocab(self, df):\n",
    "        for mod in self.getVocab():\n",
    "            df = mod.transform(df)\n",
    "        return df\n",
    "\n",
    "    #def transform(self, df, mod):\n",
    "    #    df_cols = df.columns\n",
    "        \n",
    "    #    input_col = mod.getOrDefault(mod.getParam('inputCol'))\n",
    "    #    output_col = mod.getOrDefault(mod.getParam('outputCol'))\n",
    "    #    df = mod.transform(df)\n",
    "    #    convert_vector = udf(lambda x: x.toArray().tolist(), ArrayType(DoubleType()))\n",
    "    #    df = df.withColumn(input_col + '_idx', convert_vector(col(output_col)))\n",
    "    #    df_cols.extend([input_col + '_idx'])\n",
    "\n",
    "    #    self.df = df\n",
    "\n",
    "    #    return df.select(df_cols)\n",
    "\n",
    "def Save_Model_Vocab(model, output):\n",
    "    vocab_list = model.vocabulary\n",
    "\n",
    "    int_dict = dict((i, token) for i, token in enumerate(vocab_list))\n",
    "    token_dict = {token: i for i, token in int_dict.items()}\n",
    "\n",
    "    make_directory(\"vocab\")\n",
    "    with open(\"vocab\\\\\" + output + \"_model_int-token.txt\", 'w') as file:\n",
    "        file.write(json.dumps(int_dict))\n",
    "\n",
    "    with open(\"vocab\\\\\" + output + \"_model_token-int.txt\", 'w') as file:\n",
    "        file.write(json.dumps(token_dict))\n",
    "\n",
    "    return {\"int\":int_dict, \"token\":token_dict}\n",
    "\n",
    "def make_directory(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Program</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Collect_Raw_Data_Single(sqlContext)\n",
    "df = df.withColumn(\"complaint_tokens_array\", split(col(\"complaint_tokens_str\"),' '))\n",
    "#df_Final = df.select('complaint_tokens_array').limit(5000)\n",
    "df = df.where(col(\"complaint_tokens_array\").isNotNull())\n",
    "##df.select('PATIENT_NOTE_KEY', 'complaint_tokens_str').coalesce(1).write.format('csv').mode(\"overwrite\").options(delimiter='|').save('E:/TEMP/OUTPUT/ChiefComplaint_SingleFile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+----------------------+\n",
      "|PATIENT_NOTE_KEY|complaint_tokens_str|complaint_tokens_array|\n",
      "+----------------+--------------------+----------------------+\n",
      "|        14595210|persist congest p...|  [persist, congest...|\n",
      "|        15098556|symptom much impr...|  [symptom, much, i...|\n",
      "|        14611743|week histori pain...|  [week, histori, p...|\n",
      "|        17626919|left ear pierc mo...|  [left, ear, pierc...|\n",
      "|        14873956|uri sx cough whee...|  [uri, sx, cough, ...|\n",
      "+----------------+--------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"complaint_tokens_array\", outputCol=\"complaint_tokens_vectors\", vocabSize=5000, minDF=1.0)\n",
    "#comp_cv = cv.fit(df)\n",
    "model = cv.fit(df)\n",
    "\n",
    "df_vec = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "667643"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vec.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.ones(70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+----------------------+------------------------+\n",
      "|PATIENT_NOTE_KEY|complaint_tokens_str|complaint_tokens_array|complaint_tokens_vectors|\n",
      "+----------------+--------------------+----------------------+------------------------+\n",
      "|        14595210|persist congest p...|  [persist, congest...|    (5000,[0,6,15,19,...|\n",
      "|        15098556|symptom much impr...|  [symptom, much, i...|    (5000,[1,44,61,75...|\n",
      "|        14611743|week histori pain...|  [week, histori, p...|    (5000,[0,2,9,10,3...|\n",
      "|        17626919|left ear pierc mo...|  [left, ear, pierc...|    (5000,[25,42,43,5...|\n",
      "|        14873956|uri sx cough whee...|  [uri, sx, cough, ...|    (5000,[3,9,30,34,...|\n",
      "+----------------+--------------------+----------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vec.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xs = np.array(df_vec.select('complaint_tokens_vectors').limit(5000).collect())#convert to 1d array\n",
    "#rows, d, features = Xs.shape\n",
    "splits = df_vec.randomSplit(np.ones(70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9501"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERT TO A DENSE VECTOR ONLY ON THE SPLITS TO AVOID OUF MEMORY\n",
    "from pyspark.sql.types import *\n",
    "array_udf = udf(lambda vector: vector.toArray().tolist(), ArrayType(DoubleType()))\n",
    "\n",
    "split_vec = splits[0].withColumn(\"vectors\", array_udf('complaint_tokens_vectors'))\n",
    "values = split_vec.select('vectors').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = np.array(values)\n",
    "rows, d, features = Xs.shape\n",
    "\n",
    "X = Xs.reshape(rows,features)\n",
    "X[X > 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PENDING SAVE X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
